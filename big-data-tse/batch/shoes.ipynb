{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing shoe compositions\n",
    "\n",
    "The goal of this notebook is to look at some practical NLP code, understand the concept of caching, and get familiar with working with JSON objects. We'll be using the data provided in [this](https://maxhalford.github.io/blog/carbonfact-nlp-open-problem/) blog post.\n",
    "\n",
    "## The data\n",
    "\n",
    "Let's start looking at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "inputs = pathlib.Path('shoes/inputs.txt').read_text().splitlines()\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "outputs = json.loads(pathlib.Path('shoes/outputs.json').read_text())\n",
    "len(outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a little script to print out a random sample. This will help us in getting familiar with the parsing task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è INPUT\n",
      "\n",
      "52%polyamide,39%cotton,9%elastane\n",
      "\n",
      "‚¨ÜÔ∏è OUTPUT\n",
      "\n",
      "{\n",
      "    \"\": [\n",
      "        {\n",
      "            \"material\": \"polyamide\",\n",
      "            \"proportion\": 52.0\n",
      "        },\n",
      "        {\n",
      "            \"material\": \"cotton\",\n",
      "            \"proportion\": 39.0\n",
      "        },\n",
      "        {\n",
      "            \"material\": \"elastane\",\n",
      "            \"proportion\": 9.0\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "i = random.randint(0, len(inputs))\n",
    "print(\"‚¨áÔ∏è INPUT\")\n",
    "print()\n",
    "print(inputs[i])\n",
    "print()\n",
    "print(\"‚¨ÜÔ∏è OUTPUT\")\n",
    "print()\n",
    "print(json.dumps(outputs[i], indent=4, sort_keys=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first parser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's attempt to write a first parser. We'll be using the [`regex` library](https://github.com/mrabarnett/mrab-regex), which is not part of the standard library. You'll have to install it:\n",
    "\n",
    "```sh\n",
    "pip install regex\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "def named_pattern(name, pattern):\n",
    "    return f\"(?P<{name}>{pattern})\"\n",
    "\n",
    "\n",
    "def many(pattern, at_least_one=True):\n",
    "    return f\"({pattern})+\" if at_least_one else f\"({pattern})*\"\n",
    "\n",
    "\n",
    "def sep(pattern, sep):\n",
    "    return pattern + many(sep + pattern, at_least_one=False)\n",
    "\n",
    "\n",
    "def split_composition_into_components(text):\n",
    "    \"\"\"\n",
    "\n",
    "    >>> split_composition_into_components(\"Upper: 80% Leather, 20% Textile\")\n",
    "    {'Upper': '80% Leather, 20% Textile'}\n",
    "\n",
    "    \"\"\"\n",
    "    component = \"\"\n",
    "    materials = []\n",
    "    component_materials = {}\n",
    "\n",
    "    for token in regex.split(r\"\\s+\", text):\n",
    "        if token.endswith(\":\"):\n",
    "            if materials:\n",
    "                component_materials[component] = \" \".join(materials)\n",
    "                materials = []\n",
    "            component = token.rstrip(\":\")\n",
    "        else:\n",
    "            materials.append(token)\n",
    "    else:\n",
    "        if materials:\n",
    "            component_materials[component] = \" \".join(materials)\n",
    "\n",
    "    return component_materials\n",
    "\n",
    "\n",
    "def parse_materials(text):\n",
    "    \"\"\"\n",
    "\n",
    "    >>> parse_materials(\"80% Leather 20% Textile\")\n",
    "    [{'material': 'Leather', 'proportion': 80.0}, {'material': 'Textile', 'proportion': 20.0}]\n",
    "\n",
    "    \"\"\"\n",
    "    material_pat = named_pattern(\"material\", r\"[a-zA-Z√Ä-√ø\\-\\s']+[a-zA-Z√Ä-√ø\\-']\")\n",
    "    proportion_pat = named_pattern(\"proportion\", r\"\\d{1,3}([,\\.]\\d{1,2})?\") + \"%?\"\n",
    "\n",
    "    pattern = sep(rf\"{proportion_pat}\\s*{material_pat}\", \" \")\n",
    "    match = regex.match(pattern, text)\n",
    "\n",
    "    if not match:\n",
    "        return []\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"material\": m,\n",
    "            \"proportion\": float(p)\n",
    "        }\n",
    "        for m, p in zip(\n",
    "            match.capturesdict()[\"material\"],\n",
    "            match.capturesdict()[\"proportion\"],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "\n",
    "def parse_composition(text):\n",
    "    component_materials = split_composition_into_components(text)\n",
    "    return {\n",
    "        component: parse_materials(materials)\n",
    "        for component, materials in component_materials.items()\n",
    "    }\n",
    "\n",
    "\n",
    "parsings = []\n",
    "for inp in inputs:\n",
    "    parsing = parse_composition(inp)\n",
    "    parsings.append(parsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 correct out of 600 (22.00%)\n"
     ]
    }
   ],
   "source": [
    "def performance_report(parsings, outputs):\n",
    "    n_correct = sum(parsing == output for parsing, output in zip(parsings, outputs))\n",
    "    return f\"{n_correct} correct out of {len(parsings)} ({n_correct / len(parsings) * 100:.2f}%)\"\n",
    "\n",
    "print(performance_report(parsings, outputs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ü§î Question: even though this first parser is not great, what would you say are its pros?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÄ Sidebar: have you noticed the comments under some of the functions? These are [docstrings](https://realpython.com/documenting-python-code/). In particular, these documents are using `>>>`, which is indicative of [doctests](https://docs.python.org/3/library/doctest.html). These are code comments which act as documentation by showing how the function can be used. These lines of code can also be tested, as so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestResults(failed=0, attempted=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import doctest\n",
    "\n",
    "doctest.testmod()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the command-line, you could also execute these doctests with `pytest`:\n",
    "\n",
    "```sh\n",
    "pytest --doctest-modules\n",
    "```\n",
    "\n",
    "## Looking at false positives\n",
    "\n",
    "Anyway, back to our parsing task. The first thing we should is look at false positives: samples where the parsing was incorrect. The more false positives we look at, the more we'll understand where our parser is going wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è INPUT\n",
      "\n",
      "72%polyamide,19%cotton,9%elastane\n",
      "\n",
      "‚ùå PARSING\n",
      "\n",
      "{\n",
      "    \"\": [\n",
      "        {\n",
      "            \"material\": \"polyamide\",\n",
      "            \"proportion\": 72.0\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "‚úÖ EXPECTED OUTPUT\n",
      "\n",
      "{\n",
      "    \"\": [\n",
      "        {\n",
      "            \"material\": \"polyamide\",\n",
      "            \"proportion\": 72.0\n",
      "        },\n",
      "        {\n",
      "            \"material\": \"cotton\",\n",
      "            \"proportion\": 19.0\n",
      "        },\n",
      "        {\n",
      "            \"material\": \"elastane\",\n",
      "            \"proportion\": 9.0\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "incorrect_parsings = [\n",
    "    (inp, parsing, output)\n",
    "    for inp, parsing, output in zip(inputs, parsings, outputs)\n",
    "    if parsing != output\n",
    "]\n",
    "\n",
    "i = random.randint(0, len(incorrect_parsings))\n",
    "inp, parsing, output = incorrect_parsings[i]\n",
    "print(\"‚¨áÔ∏è INPUT\")\n",
    "print()\n",
    "print(inp)\n",
    "print()\n",
    "print(\"‚ùå PARSING\")\n",
    "print()\n",
    "print(json.dumps(parsing, indent=4, sort_keys=True))\n",
    "print()\n",
    "print(\"‚úÖ EXPECTED OUTPUT\")\n",
    "print()\n",
    "print(json.dumps(output, indent=4, sort_keys=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the parser\n",
    "\n",
    "After looking at a few cases, it seems that `parse_materials` is only capturing the first material in some cases. This is because it is assuming the materials are separated with a blank space, but doesn't handle commas. Let's fix that. Usually, our code would be in a script, and we would edit it in place. For this tutorial, we'll just copy/paste the `parse_materials` function and edit it here. It's worth spending some time thinking about how you would this set this up for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_materials(text):\n",
    "    material_pat = named_pattern(\"material\", r\"[a-zA-Z√Ä-√ø\\-\\s']+[a-zA-Z√Ä-√ø\\-']\")\n",
    "    proportion_pat = named_pattern(\"proportion\", r\"\\d{1,3}([,\\.]\\d{1,2})?\") + \"%?\"\n",
    "\n",
    "    pattern = sep(rf\"{proportion_pat}\\s*{material_pat}\", \"[,\\s]\")\n",
    "    match = regex.match(pattern, text)\n",
    "\n",
    "    if not match:\n",
    "        return []\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"material\": m,\n",
    "            \"proportion\": float(p)\n",
    "        }\n",
    "        for m, p in zip(\n",
    "            match.capturesdict()[\"material\"],\n",
    "            match.capturesdict()[\"proportion\"],\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reparse all the inputs and check the new performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306 correct out of 600 (51.00%)\n"
     ]
    }
   ],
   "source": [
    "parsings = []\n",
    "for inp in inputs:\n",
    "    parsing = parse_composition(inp)\n",
    "    parsings.append(parsing)\n",
    "\n",
    "print(performance_report(parsings, outputs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ü§î Question: we reparsed all the inputs, not only the false positives. Why is that important?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing tasks are very common in the data science world. Indeed, a data scientist's job usually starts by ingesting data from several sources. It's often the case that the data is messy and needs scrubbing. Parsing structured data from text is a typical task to perform. Having a good setup where you can change code and get quick feedback about the impact of said change is a game-changer. This is referred to as a \"human-in-the-loop\" setup."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online processing\n",
    "\n",
    "The dataset is quite small, it only contains 600 cases. NLP datasets can be much larger. Let's talk a little bit about we would handle a larger corpus.\n",
    "\n",
    "The first thing we can do is online processing. Our parsing logic is a pure function which doesn't have to be trained on a dataset. In other words, it's unsupervised. What this entails is that the parsing function can be applied to each input individually. In other words, we don't need all the inputs loaded in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306 correct out of 600 (51.00%)\n"
     ]
    }
   ],
   "source": [
    "parsings = []\n",
    "\n",
    "with open('shoes/inputs.txt') as inputs:\n",
    "    for inp in inputs:\n",
    "        parsing = parse_composition(inp)\n",
    "        parsings.append(parsing)\n",
    "\n",
    "print(performance_report(parsings, outputs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got exactly the same performance, which is expected. The difference is that we could handle large datasets without clogging the RAM.\n",
    "\n",
    "Actually, our logic could still be improved. What we just did in the previous cell is great in that we read the inputs one by one. However, we are storing the parsings in a list which grows with time. We are also using `outputs`, which is list stored in memory. Ideally, we should be measuring the performance online too.\n",
    "\n",
    "One of the issues with the JSON file format is that it is not made for reading one entry at a time. That's a pretty big con, considering that JSON is ubiquitous in the machine learning world. Thankfully, there are other formats which provide the best of both worlds. For instance, there is the [JSON Lines](https://jsonlines.org/) format, which simply stores each entry in a JSON array on a row.\n",
    "\n",
    "You could say that JSON Lines isn't a new format; it's more of a convention. Nowadays, large NLP datasets are often provided as `.jsonl` files. In our case, we'll have to build that file ourselves. There are Python libraries to make this process easier -- e.g. [`jsonlines`](https://jsonlines.readthedocs.io/en/latest/) -- but it's pretty straightforward to work with without extra dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shoes/outputs.jsonl', 'w') as f:\n",
    "    for output in outputs:\n",
    "        f.write(json.dumps(output) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"lace\": [{\"material\": \"nylon\", \"proportion\": 88.0}, {\"material\": \"spandex\", \"proportion\": 12.0}], \"string\": [{\"material\": \"nylon\", \"proportion\": 88.0}, {\"material\": \"spandex\", \"proportion\": 12.0}], \"top_body\": [{\"material\": \"polyester\", \"proportion\": 100.0}]}\n",
      "{\"\": [{\"material\": \"polyester\", \"proportion\": 92.0}, {\"material\": \"spandex\", \"proportion\": 8.0}]}\n",
      "{\"\": [{\"material\": \"rayon\", \"proportion\": 95.0}, {\"material\": \"spandex\", \"proportion\": 5.0}]}\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 shoes/outputs.jsonl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can refactor our parsing logic to do everything online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306 correct out of 600 (51.00%)\n"
     ]
    }
   ],
   "source": [
    "with (\n",
    "    open('shoes/inputs.txt') as inputs_stream,\n",
    "    open('shoes/outputs.jsonl') as outputs_stream   \n",
    "):\n",
    "    n_samples = 0\n",
    "    n_correct = 0\n",
    "    for inp, output in zip(inputs_stream, outputs_stream):\n",
    "        n_samples += 1 \n",
    "        output = json.loads(output)\n",
    "        parsing = parse_composition(inp)\n",
    "        n_correct += parsing == output\n",
    "\n",
    "print(f\"{n_correct} correct out of {n_samples} ({n_correct / n_samples * 100:.2f}%)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script consumes the smallest amount of memory possible. The parsing as well as the performance tracking are done online."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ü§î Question: if we wanted to improve our parsing logic, what issue would we now face?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is quite small. Regardless of how we process it, the results can be obtained quite fast. But what if the dataset were much larger? Let's artificially increase the size of the input and output files. We'll do that by processing them several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306000 correct out of 600000 (51.00%)\n"
     ]
    }
   ],
   "source": [
    "n_samples = 0\n",
    "n_correct = 0\n",
    "\n",
    "for _ in range(1000):\n",
    "    with (\n",
    "        open('shoes/inputs.txt') as inputs_stream,\n",
    "        open('shoes/outputs.jsonl') as outputs_stream   \n",
    "    ):\n",
    "        for inp, output in zip(inputs_stream, outputs_stream):\n",
    "            n_samples += 1 \n",
    "            output = json.loads(output)\n",
    "            parsing = parse_composition(inp)\n",
    "            n_correct += parsing == output\n",
    "\n",
    "print(f\"{n_correct} correct out of {n_samples} ({n_correct / n_samples * 100:.2f}%)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's normal that this is taking longer. Processing time should be directly correlated with the amount of data. However, the above case is an illustration of repetitive computation. Each time we loop on a file, we repeat the same computation.\n",
    "\n",
    "A nice trick to know about is caching. The idea is simple: store the output after processing each input, and check if an input has already been processed before processing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306000 correct out of 600000 (51.00%)\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "@functools.cache\n",
    "def cached_parse_composition(text):\n",
    "    return parse_composition(text)\n",
    "\n",
    "n_samples = 0\n",
    "n_correct = 0\n",
    "\n",
    "for _ in range(1000):\n",
    "    with (\n",
    "        open('shoes/inputs.txt') as inputs_stream,\n",
    "        open('shoes/outputs.jsonl') as outputs_stream   \n",
    "    ):\n",
    "        for inp, output in zip(inputs_stream, outputs_stream):\n",
    "            n_samples += 1 \n",
    "            output = json.loads(output)\n",
    "            parsing = cached_parse_composition(inp)\n",
    "            n_correct += parsing == output\n",
    "\n",
    "print(f\"{n_correct} correct out of {n_samples} ({n_correct / n_samples * 100:.2f}%)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much faster! By default, `functools.cache` caches all the results. However, there are options to set a memory budget if necessary.\n",
    "\n",
    "Caching is one of the oldest tricks in the book: don't repeat what you already know."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON handling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude this tutorial, let's talk a little about handling JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = json.loads(pathlib.Path('shoes/outputs.json').read_text())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we wanted to find the top material for each component. We could do that with some Python logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lace üëâ nylon (82)\n",
      "string üëâ nylon (2)\n",
      "top_body üëâ polyester (5)\n",
      " üëâ polyamide (185)\n",
      "mesh üëâ elastane (42)\n",
      "body_panty üëâ nylon (1)\n",
      "body üëâ spandex (83)\n",
      "fabric üëâ elastane (25)\n",
      "forro üëâ polyester (2)\n",
      "ruffle üëâ polyester (1)\n",
      "elastic üëâ polyester (9)\n",
      "bottom üëâ polyester (8)\n",
      "top üëâ spandex (8)\n",
      "pant üëâ spandex (6)\n",
      "tank üëâ spandex (4)\n",
      "lining üëâ polyester (9)\n",
      "micro üëâ elastane (6)\n",
      "marl_fabric üëâ polyester (1)\n",
      "crochet üëâ polyester (2)\n",
      "liner üëâ polyester (3)\n",
      "panty üëâ nylon (1)\n",
      "edge_lace üëâ nylon (2)\n",
      "centre_front_and_wings üëâ polyamide (1)\n",
      "cup_lining üëâ polyester (2)\n",
      "cup_shell üëâ polyamide (1)\n",
      "back_panel üëâ polyamide (1)\n",
      "front_panel üëâ polyamide (1)\n",
      "cami üëâ polyester (2)\n",
      "short üëâ rayon (2)\n",
      "gusset üëâ cotton (4)\n",
      "g-string üëâ polyamide (6)\n",
      "pants üëâ polyester knitted (1)\n",
      "rib üëâ cotton (3)\n",
      "aol üëâ polyamide (2)\n",
      "shell üëâ polyester (1)\n",
      "ank üëâ rayon (1)\n",
      "knitted_top üëâ cotton woven top (1)\n",
      "trim_lace üëâ polyamide (1)\n",
      "striped_mesh üëâ polyamide (1)\n"
     ]
    }
   ],
   "source": [
    "component_counts = collections.defaultdict(collections.Counter)\n",
    "\n",
    "for output in outputs:\n",
    "    for component, materials in output.items():\n",
    "        for material in materials:\n",
    "            component_counts[component][material['material']] += 1\n",
    "\n",
    "for component, counts in component_counts.items():\n",
    "    material, count = counts.most_common(1)[0]\n",
    "    print(f\"{component} üëâ {material} ({count})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was of doing analysis is very imperative. It's probably easy to follow the logic, but it's not the canonical way of doing data analysis. A good idea is to convert the data to a table, allowing us to use a grammar with which we're more familiar.\n",
    "\n",
    "If you're lucky with the layout of the JSON file, you can use `pd.json_normalize` or `pd.read_json` with default parameters. In our case, we have to a little bit of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>material</th>\n",
       "      <th>proportion</th>\n",
       "      <th>shoe_id</th>\n",
       "      <th>component</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nylon</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0</td>\n",
       "      <td>lace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spandex</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>lace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nylon</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spandex</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>polyester</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>top_body</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    material  proportion shoe_id component\n",
       "0      nylon        88.0       0      lace\n",
       "1    spandex        12.0       0      lace\n",
       "2      nylon        88.0       0    string\n",
       "3    spandex        12.0       0    string\n",
       "4  polyester       100.0       0  top_body"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "shoes = pd.json_normalize(\n",
    "    [\n",
    "        {\n",
    "            'shoe_id': i,\n",
    "            'component': component,\n",
    "            'materials': materials\n",
    "        }\n",
    "        for i, output in enumerate(outputs)\n",
    "        for component, materials in output.items()\n",
    "    ],\n",
    "    record_path='materials',\n",
    "    meta=['shoe_id', 'component']\n",
    ")\n",
    "shoes.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do our analysis with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component</th>\n",
       "      <th>material</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>polyamide</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>body</td>\n",
       "      <td>spandex</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>lace</td>\n",
       "      <td>nylon</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>mesh</td>\n",
       "      <td>elastane</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>fabric</td>\n",
       "      <td>elastane</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>elastic</td>\n",
       "      <td>polyester</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>lining</td>\n",
       "      <td>polyester</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>bottom</td>\n",
       "      <td>polyester</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>top</td>\n",
       "      <td>spandex</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>g-string</td>\n",
       "      <td>polyamide</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>pant</td>\n",
       "      <td>rayon</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>micro</td>\n",
       "      <td>elastane</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>top_body</td>\n",
       "      <td>polyester</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>gusset</td>\n",
       "      <td>cotton</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>tank</td>\n",
       "      <td>spandex</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>rib</td>\n",
       "      <td>elastane</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>liner</td>\n",
       "      <td>polyester</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>short</td>\n",
       "      <td>spandex</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>string</td>\n",
       "      <td>spandex</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>forro</td>\n",
       "      <td>polyester</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>crochet</td>\n",
       "      <td>elastane</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>cami</td>\n",
       "      <td>spandex</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>cup_lining</td>\n",
       "      <td>polyester</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>aol</td>\n",
       "      <td>polyamide</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>edge_lace</td>\n",
       "      <td>nylon</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>shell</td>\n",
       "      <td>polyester</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>back_panel</td>\n",
       "      <td>elastane</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>ruffle</td>\n",
       "      <td>polyester</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>front_panel</td>\n",
       "      <td>polyamide</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ank</td>\n",
       "      <td>spandex</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>striped_mesh</td>\n",
       "      <td>elastane</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>pants</td>\n",
       "      <td>polyester knitted</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>trim_lace</td>\n",
       "      <td>elastane</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>panty</td>\n",
       "      <td>nylon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>knitted_top</td>\n",
       "      <td>cotton woven top</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>cup_shell</td>\n",
       "      <td>polyamide</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>marl_fabric</td>\n",
       "      <td>elastane</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>centre_front_and_wings</td>\n",
       "      <td>polyamide</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>body_panty</td>\n",
       "      <td>spandex</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  component           material  count\n",
       "9                                    polyamide    185\n",
       "35                     body            spandex     83\n",
       "75                     lace              nylon     82\n",
       "89                     mesh           elastane     42\n",
       "56                   fabric           elastane     25\n",
       "55                  elastic          polyester      9\n",
       "84                   lining          polyester      9\n",
       "39                   bottom          polyester      8\n",
       "121                     top            spandex      8\n",
       "68                 g-string          polyamide      6\n",
       "100                    pant              rayon      6\n",
       "95                    micro           elastane      6\n",
       "122                top_body          polyester      5\n",
       "70                   gusset             cotton      4\n",
       "117                    tank            spandex      4\n",
       "105                     rib           elastane      3\n",
       "82                    liner          polyester      3\n",
       "110                   short            spandex      2\n",
       "112                  string            spandex      2\n",
       "63                    forro          polyester      2\n",
       "46                  crochet           elastane      2\n",
       "43                     cami            spandex      2\n",
       "48               cup_lining          polyester      2\n",
       "22                      aol          polyamide      2\n",
       "51                edge_lace              nylon      2\n",
       "107                   shell          polyester      1\n",
       "23               back_panel           elastane      1\n",
       "106                  ruffle          polyester      1\n",
       "65              front_panel          polyamide      1\n",
       "20                      ank            spandex      1\n",
       "113            striped_mesh           elastane      1\n",
       "102                   pants  polyester knitted      1\n",
       "124               trim_lace           elastane      1\n",
       "103                   panty              nylon      1\n",
       "71              knitted_top   cotton woven top      1\n",
       "50                cup_shell          polyamide      1\n",
       "86              marl_fabric           elastane      1\n",
       "45   centre_front_and_wings          polyamide      1\n",
       "37               body_panty            spandex      1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    shoes\n",
    "    .groupby(['component', 'material'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .sort_values('count', ascending=False)\n",
    "    .groupby('component')\n",
    "    .head(1)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55fbbcf542e06cc59ad76a1e0d5dc36ee204d6d2b704491656ee6b3487310122"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
