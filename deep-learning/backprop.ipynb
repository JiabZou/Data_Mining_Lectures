{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yet another explanation of backprop\n",
    "\n",
    "There are many tutorials on backpropagation out there. I've skimmed through a bunch of them, and overall my favorite was [this one](https://www.ritchievink.com/blog/2017/07/10/programming-a-neural-network-from-scratch/) by Ritchie Vink. I preferred because the code examples are of good quality and give a lot of leeway for improvement. [This](https://victorzhou.com/blog/intro-to-neural-networks/) blogpost by Victor Zhou also helped me develop a mental model of what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks in a nutshell\n",
    "\n",
    "A neural network is a sequence of layers. Every layer takes as input $x$ and outputs $z$. We can denote this by a function which we call $f$:\n",
    "\n",
    "$$z = f(x)$$\n",
    "\n",
    "Note that the input $x$ can be a set of features, as well as the output from another layer. In the case of a dense layer, $f$ is an affine transformation:\n",
    "\n",
    "$$z = w x + b$$\n",
    "\n",
    "When we stack layers, we are simply chaining functions:\n",
    "\n",
    "$$\\hat{y} = f(f(f(\\dots(f(x)))))$$\n",
    "\n",
    "In the case of dense layers, which are linear, chaining them essentially results in a linear function. This means that even if we have a million dense layers stacked together, we still won't be able to learn non-linear patterns such as the XOR function. To add non-linearity, we add an *activation function* after each layer. Let's call these activation functions $g$. The output from the activation functions will be called $a$.\n",
    "\n",
    "$$a = g(f(x))$$\n",
    "\n",
    "When we stack layers, our final output is:\n",
    "\n",
    "$$\\hat{y} = g(f(g(f(\\dots(g(f(x)))))))$$\n",
    "\n",
    "Of course there are many more flavors of neural networks but that's the general idea. In the case of using dense layers, we're looking to tune the weights $w$ and biases $b$. That's where backpropagation comes in.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "First of all, let's get the chain rule out of the way. Say you have a function $f$, a function $g$, and an input $x$. If we compose our functions and apply them to $x$ we get $g(f(x))$. Now say we want to find the derivative of $g$ with respect to $x$. The trick is that there the function $f$ in between $g$ and $x$. In this case we use the chain rule, which gives us:\n",
    "\n",
    "$$\\frac{\\partial g}{\\partial x} = \\frac{\\partial g}{\\partial f} \\times \\frac{\\partial f}{\\partial x}$$\n",
    "\n",
    "In other words, in order to compute $\\frac{\\partial g}{\\partial x}$, we have to compute $\\frac{\\partial g}{\\partial f}$ and $\\frac{\\partial f}{\\partial x}$ and multiply them together. The chain rule is thus just a tool that we can add to our toolkit. In the case of neural networks it's super useful because we're basically just chaining functions. \n",
    "\n",
    "Let's say we're looking at the weights of the final layer. We'll call them $w$. The output of the network is denoted as $\\hat{y}$ whilst the ground truth is $y$. We have a loss function $L$ which indicates the error between $y$ and $\\hat{y}$. To update the weights, we need to calculate the gradient of the loss function with respect to the weights:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w}$$\n",
    "\n",
    "In between $w_i$ and $L$, there is the application of the dense layer and the activation function. We can thus apply the chain rule:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\times \\frac{\\partial a}{\\partial z} \\times \\frac{\\partial z}{\\partial w}$$\n",
    "\n",
    "In the case where our loss function is the mean squared error, the derivative is:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a} = 2 \\times (a - y)$$\n",
    "\n",
    "For a sigmoid activation function, the derivative is:\n",
    "\n",
    "$$\\frac{\\partial a}{\\partial z} = \\sigma(z) (1 - \\sigma(z))$$\n",
    "\n",
    "where $\\sigma$ is in fact the sigmoid function. In the case of a dense layer, the derivative is:\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial w} = x$$\n",
    "\n",
    "We simply have to multiply all these elements together in order to obtain $\\frac{\\partial L}{\\partial w}$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = (2 \\times (a - y)) \\times (\\sigma(z) (1 - \\sigma(z))) \\times x$$\n",
    "\n",
    "Recall that $a$ is the output of the network after having been processed by the activation function. We could have as well called it $\\hat{y}$ because we're looking at the final layer, but we use $a$ because it's more generic and applies to each layer in the network. $z$ is the output of the network *before* being processed by the activation function. Note that implementation wise we thus have to keep both in memory. We can't just obtain $a$ and erase $z$.\n",
    "\n",
    "If we plug in a different activation function and/or a different loss function, then everything will still work as long as each element is differentiable. Note that if we use the identity activation function (which doesn't change the input and has a derivative of 1), then we're simply doing linear regression!\n",
    "\n",
    "Now how about the weights of the penultimate layer (the one just before the last one). Well we \"just\" have write it down using the chain rule. Here goes:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial a_3} \\times \\frac{\\partial a_3}{\\partial z_3} \\times \\frac{\\partial z_3}{\\partial a_2} \\times \\frac{\\partial a_2}{\\partial z_2} \\times \\frac{\\partial z_2}{\\partial w_2}$$\n",
    "\n",
    "We've indexed the $a$s and $z$s because we're looking at multiple layer. In this case $a_3$ is the output of the 3rd layer (we called it $a$ before) whilst $a_2$ is the output of the 2nd layer. An important thing to notice is that we're using $\\frac{\\partial L}{\\partial a_3} \\times \\frac{\\partial a_3}{\\partial z_3}$, which we already calculated previously. We can exploit this when we implement backpropagation in order to speed up our code but also make it shorter.\n",
    "\n",
    "Here is the gradients for the weights of the 1st layer:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial a_3} \\times \\frac{\\partial a_3}{\\partial z_3} \\times \\frac{\\partial z_3}{\\partial a_2} \\times \\frac{\\partial a_2}{\\partial z_2} \\times \\frac{\\partial z_2}{\\partial a_1} \\times \\frac{\\partial a_1}{\\partial z_1} \\times \\frac{\\partial z_1}{\\partial w_1}$$\n",
    "\n",
    "Again the first four elements of the product have already been computed.\n",
    "\n",
    "How about the biases $b_i$? Well in a dense layer the derivative with respect to the biases is 1 (it was $x$ with respect to the weights). For the 3rd layer this will result in:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = (2 \\times (a - y)) \\times (\\sigma(z) (1 - \\sigma(z))) \\times 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"Rectified Linear Unit (ReLU) activation function.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        z[z < 0] = 0\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(z):\n",
    "        z[z < 0] = 0\n",
    "        z[z > 0] = 1\n",
    "        return z\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(z):\n",
    "        s = Sigmoid.activation(z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "\n",
    "class Identity:\n",
    "    \"\"\"Identity activation function.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(z):\n",
    "        return np.ones_like(z)\n",
    "\n",
    "\n",
    "class MSE:\n",
    "    \"\"\"Mean Squared Error (MSE) loss function.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y_true, y_pred):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(y_true, y_pred):\n",
    "        return 2 * (y_pred - y_true)\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"Stochastic Gradient Descent (SGD).\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self, weights, gradients):\n",
    "        weights -= self.learning_rate * gradients\n",
    "\n",
    "\n",
    "class NN:\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters:\n",
    "        dimensions (tuples of ints of length n_layers)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimensions, activations, loss, optimizer):\n",
    "        self.n_layers = len(dimensions)\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Weights and biases are initiated by index. For a one hidden layer net you will have a w[1] and w[2]\n",
    "        self.w = {}\n",
    "        self.b = {}\n",
    "\n",
    "        # Activations are also initiated by index. For the example we will have activations[2] and activations[3]\n",
    "        self.activations = {}\n",
    "        for i in range(len(dimensions) - 1):\n",
    "            self.w[i + 1] = np.random.randn(dimensions[i], dimensions[i + 1]) / np.sqrt(dimensions[i])\n",
    "            self.b[i + 1] = np.zeros(dimensions[i + 1])\n",
    "            self.activations[i + 2] = activations[i]\n",
    "\n",
    "    def _feed_forward(self, X):\n",
    "        \"\"\"Executes a forward pass through the neural network.\n",
    "\n",
    "        This will return the state at each layer of the network, which includes the output of the\n",
    "        network.\n",
    "\n",
    "        Parameters:\n",
    "            X (array of shape (batch_size, n_features))\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # z = w(x) + b\n",
    "        z = {}\n",
    "\n",
    "        # a = f(z)\n",
    "        a = {1: X}  # First layer has no activations as input\n",
    "\n",
    "        for i in range(2, self.n_layers + 1):\n",
    "            z[i] = np.dot(a[i - 1], self.w[i - 1]) + self.b[i - 1]\n",
    "            a[i] = self.activations[i].activation(z[i])\n",
    "\n",
    "        return z, a\n",
    "\n",
    "    def _backprop(self, z, a, y_true):\n",
    "        \"\"\"Backpropagation.\n",
    "\n",
    "        Parameters:\n",
    "            z (dict of length n_layers - 1):\n",
    "\n",
    "                z = {\n",
    "                    2: w1 * x + b1\n",
    "                    3: w2 * (w1 * x + b1) + b2\n",
    "                    4: w3 * (w2 * (w1 * x + b1) + b2) + b3\n",
    "                    ...\n",
    "                }\n",
    "\n",
    "            a (dict of length n_layers):\n",
    "\n",
    "                a = {\n",
    "                    1: x,\n",
    "                    2: f(w1 * x + b1)\n",
    "                    3: f(w2 * (w1 * x + b1) + b2)\n",
    "                    4: f(w3 * (w2 * (w1 * x + b1) + b2) + b3)\n",
    "                    ...\n",
    "                }\n",
    "\n",
    "            y_true (array of shape (batch_size, n_targets))\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Determine the partial derivative and delta for the output layer\n",
    "        y_pred = a[self.n_layers]\n",
    "        final_activation = self.activations[self.n_layers]\n",
    "        delta = self.loss.gradient(y_true, y_pred) * final_activation.gradient(y_pred)\n",
    "        dw = np.dot(a[self.n_layers - 1].T, delta)\n",
    "\n",
    "        update_params = {\n",
    "            self.n_layers - 1: (dw, delta)\n",
    "        }\n",
    "\n",
    "        # Go through the layers in reverse order\n",
    "        for i in range(self.n_layers - 2, 0, -1):\n",
    "            delta = np.dot(delta, self.w[i + 1].T) * self.activations[i + 1].gradient(z[i + 1])\n",
    "            dw = np.dot(a[i].T, delta)\n",
    "            update_params[i] = (dw, delta)\n",
    "\n",
    "        # Update the parameters\n",
    "        for k, (dw, delta) in update_params.items():\n",
    "            self.optimizer.step(weights=self.w[k], gradients=dw)\n",
    "            self.optimizer.step(weights=self.b[k], gradients=np.mean(delta, axis=0))\n",
    "\n",
    "    def fit(self, X, y, epochs, batch_size, print_every=np.inf):\n",
    "        \"\"\"Trains the neural network.\n",
    "\n",
    "        Parameters:\n",
    "            X (array of shape (n_samples, n_features))\n",
    "            y (array of shape (n_samples, n_targets))\n",
    "            epochs (int)\n",
    "            batch_size (int)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # As a convention we expect y to be 2D, even if there is only one target to predict\n",
    "        if y.ndim == 1:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        # Go through the epochs\n",
    "        for i in range(epochs):\n",
    "\n",
    "            # Shuffle the data\n",
    "            idx = np.arange(X.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            x_ = X[idx]\n",
    "            y_ = y[idx]\n",
    "\n",
    "            # Iterate over the training data in mini-batches\n",
    "            for j in range(X.shape[0] // batch_size):\n",
    "                start = j * batch_size\n",
    "                stop = (j + 1) * batch_size\n",
    "                z, a = self._feed_forward(x_[start:stop])\n",
    "                self._backprop(z, a, y_[start:stop])\n",
    "\n",
    "            # Display the performance every print_every eooch\n",
    "            if (i + 1) % print_every == 0:\n",
    "                y_pred = self.predict(X)\n",
    "                print(f'[{i+1}] train loss: {self.loss.loss(y, y_pred)}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predicts an output for each sample in X.\n",
    "\n",
    "        Parameters:\n",
    "            X (array of shape (n_samples, n_features))\n",
    "\n",
    "        \"\"\"\n",
    "        _, a = self._feed_forward(X)\n",
    "        return a[self.n_layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boston."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] train loss: 11.796707532482444\n",
      "[20] train loss: 9.700941500985953\n",
      "[30] train loss: 9.023612069639709\n",
      "2.505495393489851\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "X, y = datasets.load_boston(return_X_y=True)\n",
    "X = preprocessing.scale(X)\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y,\n",
    "    test_size=.3,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "nn = NN(\n",
    "    dimensions=(13, 10, 1),\n",
    "    activations=(ReLU, Identity),\n",
    "    loss=MSE,\n",
    "    optimizer=SGD(learning_rate=1e-3)\n",
    ")\n",
    "nn.fit(X_train, y_train, epochs=30, batch_size=8, print_every=10)\n",
    "\n",
    "y_pred = nn.predict(X_test)\n",
    "\n",
    "print(metrics.mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] train loss: 0.008308476136280957\n",
      "[20] train loss: 0.004984925198988307\n",
      "[30] train loss: 0.004102445263740696\n",
      "[40] train loss: 0.0029634369443098745\n",
      "[50] train loss: 0.0018708680417568045\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        53\n",
      "           1       0.96      0.98      0.97        50\n",
      "           2       0.94      1.00      0.97        47\n",
      "           3       0.96      0.96      0.96        54\n",
      "           4       0.98      1.00      0.99        60\n",
      "           5       0.94      0.97      0.96        66\n",
      "           6       0.98      0.98      0.98        53\n",
      "           7       1.00      0.98      0.99        55\n",
      "           8       1.00      0.93      0.96        43\n",
      "           9       0.98      0.93      0.96        59\n",
      "\n",
      "    accuracy                           0.97       540\n",
      "   macro avg       0.98      0.97      0.97       540\n",
      "weighted avg       0.97      0.97      0.97       540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "# One-hot encode y\n",
    "y = np.eye(10)[y]\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y,\n",
    "    test_size=.3,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "nn = NN(\n",
    "    dimensions=(64, 15, 10),\n",
    "    activations=(ReLU, Sigmoid),\n",
    "    loss=MSE,\n",
    "    optimizer=SGD(learning_rate=1e-3)\n",
    ")\n",
    "nn.fit(X_train, y_train, epochs=50, batch_size=16, print_every=10)\n",
    "\n",
    "y_pred = nn.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test.argmax(1), y_pred.argmax(1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
