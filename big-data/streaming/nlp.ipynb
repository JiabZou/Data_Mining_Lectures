{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP on streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3387"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups = datasets.fetch_20newsgroups(\n",
    "    subset='all',\n",
    "    remove=['headers', 'footers', 'quotes'],\n",
    "    categories=categories\n",
    ")\n",
    "stream = list(zip(\n",
    "    newsgroups.data,\n",
    "    (newsgroups.target_names[i] for i in newsgroups.target)\n",
    "))\n",
    "len(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My point is that you set up your views as the only way to believe.  Saying \n",
      "that all eveil in this world is caused by atheism is ridiculous and \n",
      "counterproductive to dialogue in this newsgroups.  I see in your posts a \n",
      "spirit of condemnation of the atheists in this newsgroup bacause they don'\n",
      "t believe exactly as you do.  If you're here to try to convert the atheists \n",
      "here, you're failing miserably.  Who wants to be in position of constantly \n",
      "defending themselves agaist insulting attacks, like you seem to like to do?!\n",
      "I'm sorry you're so blind that you didn't get the messgae in the quote, \n",
      "everyone else has seemed to.\n",
      "alt.atheism\n"
     ]
    }
   ],
   "source": [
    "text, label = stream[0]\n",
    "print(text)\n",
    "print(label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question ðŸ¤”: compared to the [anomaly detection notebook](anomaly_detection.ipynb), what is the practical difference with this dataset?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 1,\n",
       " 'point': 1,\n",
       " 'is': 3,\n",
       " 'that': 3,\n",
       " 'you': 7,\n",
       " 'set': 1,\n",
       " 'up': 1,\n",
       " 'your': 2,\n",
       " 'views': 1,\n",
       " 'as': 2,\n",
       " 'the': 5,\n",
       " 'only': 1,\n",
       " 'way': 1,\n",
       " 'to': 8,\n",
       " 'believe': 2,\n",
       " 'saying': 1,\n",
       " 'all': 1,\n",
       " 'eveil': 1,\n",
       " 'in': 6,\n",
       " 'this': 3,\n",
       " 'world': 1,\n",
       " 'caused': 1,\n",
       " 'by': 1,\n",
       " 'atheism': 1,\n",
       " 'ridiculous': 1,\n",
       " 'and': 1,\n",
       " 'counterproductive': 1,\n",
       " 'dialogue': 1,\n",
       " 'newsgroups': 1,\n",
       " 'see': 1,\n",
       " 'posts': 1,\n",
       " 'spirit': 1,\n",
       " 'of': 3,\n",
       " 'condemnation': 1,\n",
       " 'atheists': 2,\n",
       " 'newsgroup': 1,\n",
       " 'bacause': 1,\n",
       " 'they': 1,\n",
       " 'don': 1,\n",
       " 'exactly': 1,\n",
       " 'do': 2,\n",
       " 'if': 1,\n",
       " 're': 3,\n",
       " 'here': 2,\n",
       " 'try': 1,\n",
       " 'convert': 1,\n",
       " 'failing': 1,\n",
       " 'miserably': 1,\n",
       " 'who': 1,\n",
       " 'wants': 1,\n",
       " 'be': 1,\n",
       " 'position': 1,\n",
       " 'constantly': 1,\n",
       " 'defending': 1,\n",
       " 'themselves': 1,\n",
       " 'agaist': 1,\n",
       " 'insulting': 1,\n",
       " 'attacks': 1,\n",
       " 'like': 2,\n",
       " 'seem': 1,\n",
       " 'sorry': 1,\n",
       " 'so': 1,\n",
       " 'blind': 1,\n",
       " 'didn': 1,\n",
       " 'get': 1,\n",
       " 'messgae': 1,\n",
       " 'quote': 1,\n",
       " 'everyone': 1,\n",
       " 'else': 1,\n",
       " 'has': 1,\n",
       " 'seemed': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from river import feature_extraction\n",
    "\n",
    "vectorizer = feature_extraction.BagOfWords()\n",
    "\n",
    "for text, label in stream:\n",
    "    vectorizer = vectorizer.learn_one(text)\n",
    "    vector = vectorizer.transform_one(text)\n",
    "    break\n",
    "\n",
    "vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question ðŸ¤”: what do you notice about these tokens?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my': 0.05754353376484363,\n",
       " 'point': 0.05754353376484363,\n",
       " 'is': 0.1726306012945309,\n",
       " 'that': 0.1726306012945309,\n",
       " 'you': 0.4028047363539054,\n",
       " 'set': 0.05754353376484363,\n",
       " 'up': 0.05754353376484363,\n",
       " 'your': 0.11508706752968725,\n",
       " 'views': 0.05754353376484363,\n",
       " 'as': 0.11508706752968725,\n",
       " 'the': 0.2877176688242182,\n",
       " 'only': 0.05754353376484363,\n",
       " 'way': 0.05754353376484363,\n",
       " 'to': 0.460348270118749,\n",
       " 'believe': 0.11508706752968725,\n",
       " 'saying': 0.05754353376484363,\n",
       " 'all': 0.05754353376484363,\n",
       " 'eveil': 0.05754353376484363,\n",
       " 'in': 0.3452612025890618,\n",
       " 'this': 0.1726306012945309,\n",
       " 'world': 0.05754353376484363,\n",
       " 'caused': 0.05754353376484363,\n",
       " 'by': 0.05754353376484363,\n",
       " 'atheism': 0.05754353376484363,\n",
       " 'ridiculous': 0.05754353376484363,\n",
       " 'and': 0.05754353376484363,\n",
       " 'counterproductive': 0.05754353376484363,\n",
       " 'dialogue': 0.05754353376484363,\n",
       " 'newsgroups': 0.05754353376484363,\n",
       " 'see': 0.05754353376484363,\n",
       " 'posts': 0.05754353376484363,\n",
       " 'spirit': 0.05754353376484363,\n",
       " 'of': 0.1726306012945309,\n",
       " 'condemnation': 0.05754353376484363,\n",
       " 'atheists': 0.11508706752968725,\n",
       " 'newsgroup': 0.05754353376484363,\n",
       " 'bacause': 0.05754353376484363,\n",
       " 'they': 0.05754353376484363,\n",
       " 'don': 0.05754353376484363,\n",
       " 'exactly': 0.05754353376484363,\n",
       " 'do': 0.11508706752968725,\n",
       " 'if': 0.05754353376484363,\n",
       " 're': 0.1726306012945309,\n",
       " 'here': 0.11508706752968725,\n",
       " 'try': 0.05754353376484363,\n",
       " 'convert': 0.05754353376484363,\n",
       " 'failing': 0.05754353376484363,\n",
       " 'miserably': 0.05754353376484363,\n",
       " 'who': 0.05754353376484363,\n",
       " 'wants': 0.05754353376484363,\n",
       " 'be': 0.05754353376484363,\n",
       " 'position': 0.05754353376484363,\n",
       " 'constantly': 0.05754353376484363,\n",
       " 'defending': 0.05754353376484363,\n",
       " 'themselves': 0.05754353376484363,\n",
       " 'agaist': 0.05754353376484363,\n",
       " 'insulting': 0.05754353376484363,\n",
       " 'attacks': 0.05754353376484363,\n",
       " 'like': 0.11508706752968725,\n",
       " 'seem': 0.05754353376484363,\n",
       " 'sorry': 0.05754353376484363,\n",
       " 'so': 0.05754353376484363,\n",
       " 'blind': 0.05754353376484363,\n",
       " 'didn': 0.05754353376484363,\n",
       " 'get': 0.05754353376484363,\n",
       " 'messgae': 0.05754353376484363,\n",
       " 'quote': 0.05754353376484363,\n",
       " 'everyone': 0.05754353376484363,\n",
       " 'else': 0.05754353376484363,\n",
       " 'has': 0.05754353376484363,\n",
       " 'seemed': 0.05754353376484363}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from river import feature_extraction\n",
    "\n",
    "vectorizer = feature_extraction.TFIDF()\n",
    "\n",
    "for text, label in stream:\n",
    "    vectorizer = vectorizer.learn_one(text)\n",
    "    vector = vectorizer.transform_one(text)\n",
    "    break\n",
    "\n",
    "vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question ðŸ¤”: knowing how TF-IDF works, what difference does its online variant have?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,000] Accuracy: 68.47%, MacroF1: 67.29%\n",
      "[2,000] Accuracy: 72.49%, MacroF1: 71.03%\n",
      "[3,000] Accuracy: 74.66%, MacroF1: 73.16%\n",
      "[3,387] Accuracy: 74.96%, MacroF1: 73.49%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Accuracy: 74.96%, MacroF1: 73.49%"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from river import evaluate\n",
    "from river import metrics\n",
    "from river import naive_bayes\n",
    "\n",
    "model = (\n",
    "    feature_extraction.BagOfWords() |\n",
    "    naive_bayes.MultinomialNB()\n",
    ")\n",
    "\n",
    "metric = metrics.Accuracy() + metrics.MacroF1()\n",
    "\n",
    "evaluate.progressive_val_score(stream, model, metric, print_every=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question ðŸ¤”: what makes the comparison with a batch approach difficult?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n",
      "387\n"
     ]
    }
   ],
   "source": [
    "def batch(stream, size):\n",
    "    batch = []\n",
    "    for x, y in stream:\n",
    "        batch.append((x, y))\n",
    "        if len(batch) == size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "for mini_batch in batch(stream, size=1000):\n",
    "    print(len(mini_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 17444 features, but GaussianNB is expecting 19939 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [74], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mmini_batch)\n\u001b[1;32m      9\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mfit_transform(X)\u001b[39m.\u001b[39mtoarray()\n\u001b[0;32m---> 10\u001b[0m model\u001b[39m.\u001b[39;49mpartial_fit(X, y, classes\u001b[39m=\u001b[39;49mcategories)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/sklearn/naive_bayes.py:389\u001b[0m, in \u001b[0;36mGaussianNB.partial_fit\u001b[0;34m(self, X, y, classes, sample_weight)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39m\"\"\"Incremental fit on a batch of samples.\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \n\u001b[1;32m    350\u001b[0m \u001b[39mThis method is expected to be called several times consecutively\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39m    Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m--> 389\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_partial_fit(\n\u001b[1;32m    390\u001b[0m     X, y, classes, _refit\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[1;32m    391\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/sklearn/naive_bayes.py:426\u001b[0m, in \u001b[0;36mGaussianNB._partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    425\u001b[0m first_call \u001b[39m=\u001b[39m _check_partial_fit_first_call(\u001b[39mself\u001b[39m, classes)\n\u001b[0;32m--> 426\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, reset\u001b[39m=\u001b[39;49mfirst_call)\n\u001b[1;32m    427\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/sklearn/base.py:558\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    555\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 558\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[1;32m    560\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/sklearn/base.py:359\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 359\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    360\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    361\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 17444 features, but GaussianNB is expecting 19939 features as input."
     ]
    }
   ],
   "source": [
    "from sklearn import feature_extraction\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "vectorizer = feature_extraction.text.CountVectorizer()\n",
    "model = naive_bayes.GaussianNB()\n",
    "\n",
    "for mini_batch in batch(stream, size=1000):\n",
    "    X, y = zip(*mini_batch)\n",
    "    X = vectorizer.fit_transform(X).toarray()\n",
    "    model.partial_fit(X, y, classes=categories)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question ðŸ¤”: what is the issue?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way of dealing with a varying number of features is called the [\"hashing trick\"](https://www.wikiwand.com/en/Feature_hashing). scikit-learn has a `HashingVectorizer`, which is a combination of [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [`FeatureHasher`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "vectorizer = feature_extraction.text.HashingVectorizer(n_features=2000)\n",
    "model = naive_bayes.GaussianNB()\n",
    "\n",
    "for mini_batch in batch(stream, size=1000):\n",
    "    X, y = zip(*mini_batch)\n",
    "    X = vectorizer.fit_transform(X).toarray()\n",
    "    model.partial_fit(X, y, classes=categories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55fbbcf542e06cc59ad76a1e0d5dc36ee204d6d2b704491656ee6b3487310122"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
