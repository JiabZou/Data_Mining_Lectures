{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM deep dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why LightGBM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tree boosting works well:\n",
    "    - Trees usually work well on tabular data\n",
    "      - [Why do tree-based models still outperform deep learning on tabular data?](https://arxiv.org/abs/2207.08815)\n",
    "      - [When Do Neural Nets Outperform Boosted Trees on Tabular Data?](https://arxiv.org/abs/2305.02997)\n",
    "      - [Machine Learning Challenge Winning Solutions](https://github.com/microsoft/LightGBM/tree/master/examples#machine-learning-challenge-winning-solutions)\n",
    "    - Not that many parameters to tune:\n",
    "      - Tree height\n",
    "      - Number of trees\n",
    "      - Learning rate\n",
    "- Added benefits of LightGBM:\n",
    "    - Fast\n",
    "    - Supports categorical variables\n",
    "    - Handles missing data\n",
    "    - Can output prediction intervals\n",
    "    - No need to do feature selection\n",
    "    - Provides feature importance\n",
    "    - Works for regression, classification, ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/code/prashant111/lightgbm-classifier-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset we produced in the previous notebook. The one about taxi trip durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>l1_distance</th>\n",
       "      <th>l2_distance</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>avg_duration_per_hour_recent</th>\n",
       "      <th>avg_duration_per_weekday_recent</th>\n",
       "      <th>avg_duration_recent</th>\n",
       "      <th>pickup_cell_id</th>\n",
       "      <th>dropoff_cell_id</th>\n",
       "      <th>cell_pair_count</th>\n",
       "      <th>avg_duration_per_cell_pair</th>\n",
       "      <th>avg_duration_per_hour_per_cell_pair</th>\n",
       "      <th>avg_duration_per_weekday_per_cell_pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id0190469</td>\n",
       "      <td>2016-01-01 00:00:17</td>\n",
       "      <td>849</td>\n",
       "      <td>0.152939</td>\n",
       "      <td>0.118097</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2-7</td>\n",
       "      <td>6-18</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id1665586</td>\n",
       "      <td>2016-01-01 00:00:53</td>\n",
       "      <td>1294</td>\n",
       "      <td>0.056721</td>\n",
       "      <td>0.040151</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>849.000000</td>\n",
       "      <td>849.000000</td>\n",
       "      <td>849.000000</td>\n",
       "      <td>2-10</td>\n",
       "      <td>4-7</td>\n",
       "      <td>211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id1210365</td>\n",
       "      <td>2016-01-01 00:01:01</td>\n",
       "      <td>408</td>\n",
       "      <td>0.031929</td>\n",
       "      <td>0.022726</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1071.500000</td>\n",
       "      <td>1071.500000</td>\n",
       "      <td>1071.500000</td>\n",
       "      <td>4-15</td>\n",
       "      <td>5-17</td>\n",
       "      <td>233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3888279</td>\n",
       "      <td>2016-01-01 00:01:14</td>\n",
       "      <td>280</td>\n",
       "      <td>0.010040</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>850.333333</td>\n",
       "      <td>850.333333</td>\n",
       "      <td>850.333333</td>\n",
       "      <td>2-10</td>\n",
       "      <td>2-10</td>\n",
       "      <td>7392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id0924227</td>\n",
       "      <td>2016-01-01 00:01:20</td>\n",
       "      <td>736</td>\n",
       "      <td>0.036060</td>\n",
       "      <td>0.025557</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>707.750000</td>\n",
       "      <td>707.750000</td>\n",
       "      <td>707.750000</td>\n",
       "      <td>3-11</td>\n",
       "      <td>2-10</td>\n",
       "      <td>11054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     pickup_datetime  trip_duration  l1_distance  l2_distance  \\\n",
       "0  id0190469 2016-01-01 00:00:17            849     0.152939     0.118097   \n",
       "1  id1665586 2016-01-01 00:00:53           1294     0.056721     0.040151   \n",
       "2  id1210365 2016-01-01 00:01:01            408     0.031929     0.022726   \n",
       "3  id3888279 2016-01-01 00:01:14            280     0.010040     0.009103   \n",
       "4  id0924227 2016-01-01 00:01:20            736     0.036060     0.025557   \n",
       "\n",
       "  hour weekday  avg_duration_per_hour_recent  avg_duration_per_weekday_recent  \\\n",
       "0    0       5                           NaN                              NaN   \n",
       "1    0       5                    849.000000                       849.000000   \n",
       "2    0       5                   1071.500000                      1071.500000   \n",
       "3    0       5                    850.333333                       850.333333   \n",
       "4    0       5                    707.750000                       707.750000   \n",
       "\n",
       "   avg_duration_recent pickup_cell_id dropoff_cell_id  cell_pair_count  \\\n",
       "0                  NaN            2-7            6-18               18   \n",
       "1           849.000000           2-10             4-7              211   \n",
       "2          1071.500000           4-15            5-17              233   \n",
       "3           850.333333           2-10            2-10             7392   \n",
       "4           707.750000           3-11            2-10            11054   \n",
       "\n",
       "   avg_duration_per_cell_pair  avg_duration_per_hour_per_cell_pair  \\\n",
       "0                         NaN                                  NaN   \n",
       "1                         NaN                                  NaN   \n",
       "2                         NaN                                  NaN   \n",
       "3                         NaN                                  NaN   \n",
       "4                         NaN                                  NaN   \n",
       "\n",
       "   avg_duration_per_weekday_per_cell_pair  \n",
       "0                                     NaN  \n",
       "1                                     NaN  \n",
       "2                                     NaN  \n",
       "3                                     NaN  \n",
       "4                                     NaN  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_pickle('../../data/taxi_trip_dataset.pkl')\n",
    "dataset['hour'] = pd.Categorical(dataset['hour'])\n",
    "dataset['weekday'] = pd.Categorical(dataset['weekday'])\n",
    "dataset['pickup_cell_id'] = pd.Categorical(dataset['pickup_cell_id'])\n",
    "dataset['dropoff_cell_id'] = pd.Categorical(dataset['dropoff_cell_id'])\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict the trip duration of a taxi ride in New York City given the pickup and dropoff locations. We will use LightGBM for this task. As with any supervised task, the goal is to learn a model on a training set and then make predictions on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_test = dataset['pickup_datetime'].dt.month == 6\n",
    "X_train = dataset.loc[~is_test].drop(columns=['id', 'trip_duration', 'pickup_datetime'])\n",
    "y_train = dataset.loc[~is_test, 'trip_duration']\n",
    "X_test = dataset.loc[is_test].drop(columns=['id', 'trip_duration', 'pickup_datetime'])\n",
    "y_test = dataset.loc[is_test, 'trip_duration']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'len(X_train)=1,219,608, len(X_test)=233,460'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{len(X_train)=:,d}, {len(X_test)=:,d}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to fit a model on `(X_train, y_train)`, and then make predictions `y_pred` on `X_test`. We want to minimize the mean squared error between `y_pred` and `y_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mlu-explain.github.io/decision-tree/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting (for trees)\n",
    "\n",
    "Gradient boosting is a general purpose algorithm for regression and classification. It can be used with any differentiable loss function. It is an ensemble method that combines weak learners to create a strong learner. It works well with decision trees as weak learners, but it can also work with other types of models.\n",
    "\n",
    "The idea is to iteratively fit a weak learner to the residuals of the previous model. The residuals are the difference between the predictions of the previous model and the true values. The weak learner is then fitted to the residuals, and the predictions of the weak learner are added to the predictions of the previous model.\n",
    "\n",
    "Let's assume we're doing regression. The algorithm is as follows:\n",
    "\n",
    "*Initialize the model with a constant value*\n",
    "\n",
    "$$\\hat{y}^0 = \\frac{1}{n} \\sum_{i=1}^n y_i$$\n",
    "\n",
    "$\\hat{y}^0$ is a vector of length $n$ with all values equal to the mean of $y$.\n",
    "\n",
    "*Calculate the residuals*\n",
    "\n",
    "$$r^0 = y - \\hat{y}^0$$\n",
    "\n",
    "$r^0$ is a vector of length $n$, where each value $r^0_i$ is the difference between the true value $y_i$ and $\\hat{y}^0_i$. In other words, it's the gradient of the loss function with respect to the predictions of the previous model.\n",
    "\n",
    "*Fit a weak learner to the residuals*\n",
    "\n",
    "$$\\hat{f}^1 = \\text{argmin}_f \\sum_{i=1}^n L(y_i, \\hat{y}^0_i + f(x_i))$$\n",
    "\n",
    "$\\hat{f}^1$ is a vector of length $n$ with the predictions of the weak learner. The weak learner is fitted to the residuals $r^0$. The weak learner learns to predict the residuals $r^0$ from the training features $x$. It therefore learns to correct the errors of the previous model.\n",
    "\n",
    "*Update the model*\n",
    "\n",
    "$$\\hat{y}^1 = \\hat{y}^0 + \\gamma \\times \\hat{f}^1$$\n",
    "\n",
    "$\\hat{y}^1$ is a vector of length $n$ with the predictions of the model after the first iteration. The predictions of the weak learner are added to the predictions of the previous model. The learning rate $\\gamma$ controls how much the predictions of the weak learner are added to the predictions of the previous model.\n",
    "\n",
    "*Calculate the residuals*\n",
    "\n",
    "$$r^1 = y - \\hat{y}^1$$\n",
    "\n",
    "$r^1$ is a vector of length $n$, where each value $r^1_i$ is the difference between the true value $y_i$ and $\\hat{y}^1_i$.\n",
    "\n",
    "*Fit a weak learner to the residuals*\n",
    "\n",
    "$$\\hat{f}^2 = \\text{argmin}_f \\sum_{i=1}^n L(y_i, \\hat{y}^1_i + f(x_i))$$\n",
    "\n",
    "$\\hat{f}^2$ is a vector of length $n$ with the predictions of the weak learner. The weak learner is fitted to the residuals $r^1$. The weak learner learns to predict the residuals $r^1$ from the training features $x$.\n",
    "\n",
    "*Update the model*\n",
    "\n",
    "$$\\hat{y}^2 = \\hat{y}^1 + \\gamma \\times \\hat{f}^2$$\n",
    "\n",
    "$$\\hat{y}^2 = \\hat{y}^0 + \\gamma \\times \\hat{f}^1 + \\gamma \\times \\hat{f}^2$$\n",
    "\n",
    "The idea is to keep doing this for a certain number of iterations. The more iterations, the more $\\hat{y}$ will approach $y$. This is good because it means the model is learning. However, if we do too many iterations, the model will overfit the training data. It will learn the noise in the training data, and it will not generalize well to the test data. This is why we need to tune the number of iterations. We can do this with early stopping, which involves monitoring the performance of the model on a validation set. If the performance on the validation set does not improve for a certain number of iterations, we stop the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works with some code. We'll use a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Generate the original dataset\n",
    "X, y = datasets.make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_fit, X_val, y_fit, y_val = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Introduce a different distribution in the validation set by adding noise\n",
    "noise_factor = 1  # Adjust the noise factor as needed\n",
    "X_val += np.random.normal(0, noise_factor, X_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Hyperparameters\n",
    "tree_max_depth = 10\n",
    "n_iterations = 30\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize model parameters\n",
    "y_fit_pred = np.full(shape=len(y_fit), fill_value=y_fit.mean())\n",
    "\n",
    "# Monitor loss values\n",
    "fit_loss_values = []\n",
    "val_loss_values = []\n",
    "\n",
    "def l2_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def l2_loss_gradient(y_true, y_pred):\n",
    "    return -2 * (y_true - y_pred)\n",
    "\n",
    "def predict(X):\n",
    "    y_pred = np.full(shape=len(X), fill_value=y_fit.mean())\n",
    "\n",
    "    for weak_learner in weak_learners:\n",
    "        y_pred += learning_rate * weak_learner.predict(X)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "weak_learners = []\n",
    "for m in range(n_iterations):\n",
    "\n",
    "    # We take the current predictions and measure the gap with the true values\n",
    "    negative_gradients = -l2_loss_gradient(y_fit, y_fit_pred)\n",
    "\n",
    "    # We fit a tree to predict the gap given the features\n",
    "    weak_learner = tree.DecisionTreeRegressor(max_depth=tree_max_depth)\n",
    "    weak_learner.fit(X_fit, y=negative_gradients)\n",
    "    y_fit_pred += learning_rate * weak_learner.predict(X_fit)\n",
    "\n",
    "    # We add the weak learner to the ensemble\n",
    "    weak_learners.append(weak_learner)\n",
    "\n",
    "    # We want to monitor the loss values\n",
    "    fit_loss_values.append(l2_loss(y_fit, predict(X_fit)))  # predict(X_fit) = y_fit_pred\n",
    "    val_loss_values.append(l2_loss(y_val, predict(X_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-62e10bffb7e24c1cb48b9bf6e498e144.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-62e10bffb7e24c1cb48b9bf6e498e144.vega-embed details,\n",
       "  #altair-viz-62e10bffb7e24c1cb48b9bf6e498e144.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-62e10bffb7e24c1cb48b9bf6e498e144\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-62e10bffb7e24c1cb48b9bf6e498e144\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-62e10bffb7e24c1cb48b9bf6e498e144\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.15.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.15.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-0b340716c69d948d1222e6e3d0fdcc58\"}, \"mark\": {\"type\": \"circle\"}, \"encoding\": {\"color\": {\"field\": \"Loss Type\", \"type\": \"nominal\"}, \"x\": {\"field\": \"Iteration\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"Loss Value\", \"type\": \"quantitative\"}}, \"height\": 400, \"params\": [{\"name\": \"param_41\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"title\": \"Fit and Validation Loss Over Iterations\", \"transform\": [{\"fold\": [\"Fit Loss\", \"Val Loss\"], \"as\": [\"Loss Type\", \"Loss Value\"]}], \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.15.1.json\", \"datasets\": {\"data-0b340716c69d948d1222e6e3d0fdcc58\": [{\"Iteration\": 1, \"Fit Loss\": 11341.083959757607, \"Val Loss\": 13039.055017927405}, {\"Iteration\": 2, \"Fit Loss\": 7306.588302077568, \"Val Loss\": 11055.217356010864}, {\"Iteration\": 3, \"Fit Loss\": 4709.007552438805, \"Val Loss\": 10299.939654333244}, {\"Iteration\": 4, \"Fit Loss\": 3034.0259442625666, \"Val Loss\": 10040.933164960581}, {\"Iteration\": 5, \"Fit Loss\": 1957.9950889838115, \"Val Loss\": 10115.928646235394}, {\"Iteration\": 6, \"Fit Loss\": 1260.2371043528035, \"Val Loss\": 10475.322704417822}, {\"Iteration\": 7, \"Fit Loss\": 814.6603273339788, \"Val Loss\": 10830.568457559515}, {\"Iteration\": 8, \"Fit Loss\": 525.9386606549418, \"Val Loss\": 11183.667588053486}, {\"Iteration\": 9, \"Fit Loss\": 340.35021171144405, \"Val Loss\": 11466.163446115419}, {\"Iteration\": 10, \"Fit Loss\": 219.94954339601142, \"Val Loss\": 11728.073227123441}, {\"Iteration\": 11, \"Fit Loss\": 141.93115097595825, \"Val Loss\": 11992.882863905998}, {\"Iteration\": 12, \"Fit Loss\": 91.62065123508157, \"Val Loss\": 12155.82983518402}, {\"Iteration\": 13, \"Fit Loss\": 59.165058215424544, \"Val Loss\": 12334.053859648453}, {\"Iteration\": 14, \"Fit Loss\": 38.151809320562144, \"Val Loss\": 12459.30325020309}, {\"Iteration\": 15, \"Fit Loss\": 24.59762001268119, \"Val Loss\": 12558.282137360775}, {\"Iteration\": 16, \"Fit Loss\": 15.902713628998699, \"Val Loss\": 12646.248545806007}, {\"Iteration\": 17, \"Fit Loss\": 10.261968556410524, \"Val Loss\": 12721.052677712609}, {\"Iteration\": 18, \"Fit Loss\": 6.629282168492723, \"Val Loss\": 12784.953225658926}, {\"Iteration\": 19, \"Fit Loss\": 4.290109625866617, \"Val Loss\": 12830.573274983393}, {\"Iteration\": 20, \"Fit Loss\": 2.7684210958862354, \"Val Loss\": 12866.761359319496}, {\"Iteration\": 21, \"Fit Loss\": 1.7973590805538118, \"Val Loss\": 12892.62049976773}, {\"Iteration\": 22, \"Fit Loss\": 1.1716484863327057, \"Val Loss\": 12912.1959075562}, {\"Iteration\": 23, \"Fit Loss\": 0.7602223189573255, \"Val Loss\": 12932.461929970714}, {\"Iteration\": 24, \"Fit Loss\": 0.4960765914541029, \"Val Loss\": 12945.133857409783}, {\"Iteration\": 25, \"Fit Loss\": 0.3212726942220037, \"Val Loss\": 12954.649665949806}, {\"Iteration\": 26, \"Fit Loss\": 0.20796922323277425, \"Val Loss\": 12962.907779538431}, {\"Iteration\": 27, \"Fit Loss\": 0.13501414475552934, \"Val Loss\": 12968.832818006806}, {\"Iteration\": 28, \"Fit Loss\": 0.08796626107833572, \"Val Loss\": 12974.750587357634}, {\"Iteration\": 29, \"Fit Loss\": 0.05708932892553701, \"Val Loss\": 12980.09863546618}, {\"Iteration\": 30, \"Fit Loss\": 0.03709978978831324, \"Val Loss\": 12983.781439277627}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt\n",
    "\n",
    "data = {\n",
    "    'Iteration': np.arange(1, len(fit_loss_values) + 1),\n",
    "    'Fit Loss': fit_loss_values,\n",
    "    'Val Loss': val_loss_values\n",
    "}\n",
    "\n",
    "chart = alt.Chart(pd.DataFrame(data)).transform_fold(\n",
    "    ['Fit Loss', 'Val Loss'],\n",
    "    as_=['Loss Type', 'Loss Value']\n",
    ").mark_circle().encode(\n",
    "    x='Iteration:O',\n",
    "    y='Loss Value:Q',\n",
    "    color='Loss Type:N'\n",
    ").properties(\n",
    "    title='Fit and Validation Loss Over Iterations',\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "chart.interactive()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, we've implemented gradient boosting from scratch! Our implementation is quite basic. Here are some things LightGBM does that we haven't implemented:\n",
    "\n",
    "- Predictive performance\n",
    "  - The weak learners use the hessian as well as the gradient of the loss function.\n",
    "  - The decision tree split finding algorithm is specifically designed for gradient boosting.\n",
    "- Throughput\n",
    "  - Samples with small gradients are not used to train the weak learners.\n",
    "  - The split finding algorithm uses a histogram approximation to speed up the computation.\n",
    "  - Exclusive feature bundling: features are grouped together to speed up the computation.\n",
    "  - Row and/or column can be used to fit each weak learner on a subset of the data.\n",
    "- Scalability\n",
    "  - The data can be distributed across multiple machines, and the learning happens in a distributed fashion.\n",
    "  - The data can be stored on disk, and the learning happens out-of-core.\n",
    "\n",
    "LightGBM's C++ code is too convoluted to understand. There is however [TinyGBT](https://github.com/lancifollia/tinygbt), which is implemented in Python and is much easier to understand. It's a good starting point if you want to understand how tree-based gradient boosting works. It is 200 lines of code and reaches the same predictive performance as LightGBM. It is of course much slower than LightGBM, but that's the price to pay for readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use LightGBM\n",
    "\n",
    "LightGBM provides a [scikit-learn API](https://lightgbm.readthedocs.io/en/stable/pythonapi/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "model = lgb.LGBMRegressor()\n",
    "\n",
    "model = model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:27.731214\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from sklearn import metrics\n",
    "\n",
    "def evaluate(model):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    print(dt.timedelta(seconds=mae))\n",
    "\n",
    "evaluate(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Hyperparameters are described in the [documentation](https://lightgbm.readthedocs.io/en/stable/Parameters.html). The most important parameters are:\n",
    "\n",
    "- `objective`: the loss function to optimize.\n",
    "- `max_depth`: the maximum depth of each tree.\n",
    "- `num_leaves`: the maximum number of leaves in each tree.\n",
    "- `n_estimators`: the number of trees.\n",
    "- `learning_rate`: the learning rate.\n",
    "- `min_child_samples`: the minimum number of samples in each leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `objective`\n",
    "\n",
    "The objective is the loss function to optimize. It's important to distinguish the loss function the model optimizes, from the metric we want to minimize. Indeed, a model can optimize a differentiable loss function, but we can still use a non-differentiable metric to evaluate the model.\n",
    "\n",
    "In our case, we're looking to minimize the mean absolute error (MAE). We're in luck, because LightGBM can minimize the L1 loss function. The L1 loss function is equivalent to the mean absolute error. It's the sum of the absolute values of the residuals divided by the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:25.826597\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor(\n",
    "    objective='regression_l1'\n",
    ")\n",
    "\n",
    "model = model.fit(X_train, y_train)\n",
    "evaluate(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `max_depth`\n",
    "\n",
    "The maximum depth of each tree is controlled by the `max_depth` parameter. The default value is `-1`, which means there is no maximum depth. The tree grows until all leaves are pure or until all leaves contain less than `min_child_samples` samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:27.044919\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor(\n",
    "    objective='regression_l1',\n",
    "    max_depth=7\n",
    ")\n",
    "\n",
    "model = model.fit(X_train, y_train)\n",
    "evaluate(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the tree depth doesn't play a big role because the train and test sets share the same distribution. The model will overfit the training data, but it will generalize well to the test data. The tree depth is a good way to control the complexity of the model. The deeper the tree, the more complex the model. The more complex the model, the more likely it is to overfit the training data.\n",
    "\n",
    "Note that controlling the tree depth is also a good way to control the training time. The deeper the tree, the longer it takes to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `num_leaves`\n",
    "\n",
    "LightGBM learns in a leaf-wise fashion. It grows the tree leaf by leaf. This is different from other tree-based models, which grow the tree level by level. It thus provides a `num_leaves` parameter, which controls the maximum number of leaves in each tree. The tree grows until all leaves are pure or until all leaves contain less than `min_child_samples` samples. This is slightly different from the `max_depth` parameter, which controls the maximum depth of each tree. Both can be used in combination to control the model's complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:17.490448\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor(\n",
    "    objective='regression_l1',\n",
    "    num_leaves=2 ** 7\n",
    ")\n",
    "\n",
    "model = model.fit(X_train, y_train)\n",
    "evaluate(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `n_estimators`\n",
    "\n",
    "The number of trees is controlled by the `n_estimators` parameter. The default value is `100`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_estimators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more trees, the more likely it is to overfit the training data. The more trees, the longer it takes to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:15.170267\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor(\n",
    "    objective='regression_l1',\n",
    "    num_leaves=2 ** 7,\n",
    "    n_estimators=500\n",
    ")\n",
    "\n",
    "model = model.fit(X_train, y_train)\n",
    "evaluate(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`learning_rate`\n",
    "\n",
    "The learning rate can be increased to speed up the training. The learning rate controls how much the predictions of the weak learner are added to the predictions of the previous model. The higher the learning rate, the more the predictions of the weak learner are added to the predictions of the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:16.098418\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor(\n",
    "    objective='regression_l1',\n",
    "    num_leaves=2 ** 7,\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.2\n",
    ")\n",
    "\n",
    "model = model.fit(X_train, y_train)\n",
    "evaluate(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "Early stopping is a technique to prevent overfitting. It involves monitoring the performance of the model on a validation set. If the performance on the validation set does not improve for a certain number of iterations, we stop the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/.pyenv/versions/3.11.0/lib/python3.11/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/Users/max/.pyenv/versions/3.11.0/lib/python3.11/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's l1: 175.537\tvalid_1's l1: 180.005\n",
      "[100]\ttraining's l1: 171.544\tvalid_1's l1: 178.254\n",
      "[150]\ttraining's l1: 169.717\tvalid_1's l1: 177.682\n",
      "Early stopping, best iteration is:\n",
      "[155]\ttraining's l1: 169.623\tvalid_1's l1: 177.676\n"
     ]
    }
   ],
   "source": [
    "X_fit, X_val, y_fit, y_val = model_selection.train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model = lgb.LGBMRegressor(\n",
    "    objective='regression_l1',\n",
    "    num_leaves=2 ** 7,\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.2\n",
    ")\n",
    "\n",
    "model = model.fit(\n",
    "    X_fit, y_fit,\n",
    "    eval_set=[\n",
    "        (X_fit, y_fit),\n",
    "        (X_val, y_val)\n",
    "    ],\n",
    "    eval_metric='l1',\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(10),\n",
    "        lgb.log_evaluation(period=50)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:16.868874\n"
     ]
    }
   ],
   "source": [
    "evaluate(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, early stopping doesn't guarantee that the model will generalize well to the test data. It only guarantees that the model will generalize well to the validation data. The validation data is a sample of the training data. It's not the test data.\n",
    "\n",
    "In practice, the main benefit of early stopping is to speed up the training. It allows us to stop the training early if the model is not improving. It's a good way to control the training time and thus iterate faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validated predictions\n",
    "\n",
    "This is a technique that developped on Kaggle. The idea is to train the model on different slices of the training data. For each slice, we train the model on a subset of the training data, and we make predictions on the test set. We then average the predictions of all the models to get the final predictions. This is form of bootstrap aggregation, or bagging for short. It reduces the variance of the predictions, and it can improve the predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/.pyenv/versions/3.11.0/lib/python3.11/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/Users/max/.pyenv/versions/3.11.0/lib/python3.11/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's l1: 175.465\tvalid_1's l1: 180.014\n",
      "[100]\ttraining's l1: 171.776\tvalid_1's l1: 178.54\n",
      "[150]\ttraining's l1: 170.165\tvalid_1's l1: 178.107\n",
      "[200]\ttraining's l1: 169.017\tvalid_1's l1: 177.866\n",
      "Early stopping, best iteration is:\n",
      "[220]\ttraining's l1: 168.735\tvalid_1's l1: 177.841\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/.pyenv/versions/3.11.0/lib/python3.11/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/Users/max/.pyenv/versions/3.11.0/lib/python3.11/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's l1: 175.374\tvalid_1's l1: 180.956\n",
      "[100]\ttraining's l1: 171.25\tvalid_1's l1: 179.148\n",
      "Early stopping, best iteration is:\n",
      "[135]\ttraining's l1: 170.174\tvalid_1's l1: 178.904\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/.pyenv/versions/3.11.0/lib/python3.11/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/Users/max/.pyenv/versions/3.11.0/lib/python3.11/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's l1: 175.415\tvalid_1's l1: 180.266\n",
      "[100]\ttraining's l1: 171.33\tvalid_1's l1: 178.538\n",
      "[150]\ttraining's l1: 169.938\tvalid_1's l1: 178.306\n",
      "[200]\ttraining's l1: 169.162\tvalid_1's l1: 178.228\n",
      "Early stopping, best iteration is:\n",
      "[191]\ttraining's l1: 169.259\tvalid_1's l1: 178.226\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/.pyenv/versions/3.11.0/lib/python3.11/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/Users/max/.pyenv/versions/3.11.0/lib/python3.11/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's l1: 175.391\tvalid_1's l1: 180.426\n",
      "[100]\ttraining's l1: 171.153\tvalid_1's l1: 178.649\n",
      "[150]\ttraining's l1: 169.458\tvalid_1's l1: 178.174\n",
      "Early stopping, best iteration is:\n",
      "[154]\ttraining's l1: 169.396\tvalid_1's l1: 178.171\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/.pyenv/versions/3.11.0/lib/python3.11/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/Users/max/.pyenv/versions/3.11.0/lib/python3.11/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's l1: 175.369\tvalid_1's l1: 180.708\n",
      "[100]\ttraining's l1: 171.036\tvalid_1's l1: 178.897\n",
      "[150]\ttraining's l1: 169.762\tvalid_1's l1: 178.71\n",
      "Early stopping, best iteration is:\n",
      "[177]\ttraining's l1: 169.094\tvalid_1's l1: 178.578\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "cv = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "model = lgb.LGBMRegressor(\n",
    "    objective='regression_l1',\n",
    "    num_leaves=2 ** 7,\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.2\n",
    ")\n",
    "\n",
    "y_pred = np.zeros(len(X_test))\n",
    "\n",
    "for i, (fit_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "    print(f'Fold {i + 1}')\n",
    "    X_fit = X_train.iloc[fit_idx]\n",
    "    y_fit = y_train.iloc[fit_idx]\n",
    "    X_val = X_train.iloc[val_idx]\n",
    "    y_val = y_train.iloc[val_idx]\n",
    "\n",
    "    model = model.fit(\n",
    "        X_fit, y_fit,\n",
    "        eval_set=[\n",
    "            (X_fit, y_fit),\n",
    "            (X_val, y_val)\n",
    "        ],\n",
    "        eval_metric='l1',\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(10),\n",
    "            lgb.log_evaluation(period=50)\n",
    "        ]\n",
    "    )\n",
    "    print('-' * 30)\n",
    "\n",
    "    y_pred += model.predict(X_test)\n",
    "\n",
    "y_pred /= cv.get_n_splits()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:14.785986\n"
     ]
    }
   ],
   "source": [
    "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "print(dt.timedelta(seconds=mae))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downside of this technique is that it increases the training time because we have to train the model multiple times.\n",
    "\n",
    "We can see that tuning the hyperparameters reduces the MAE on the test set by roughly 20 seconds. The feature engineering part managed to reduce the MAE on the test set by about 45 seconds. That's almost always the case in practice: feature engineering reaps more benefits than hyperparameter tuning.\n",
    "\n",
    "We could do some hyperparameter tuning to tune the amount of leaves, the learning rate, the number of estimators, etc. However, it's unlikely to improve the predictive performance by much. Also, this would be extremely costly if we want to do it with bagging. My advice is to spend a bit of time searching for \"good enough\" hyperparameter. Then, spend most of your time on feature engineering. I know this sounds a bit more like art than science, but that's the reality of machine learning in practice: we don't have unlimited time and resources to tune the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(\n",
    "    objective='regression_l1',\n",
    "    num_leaves=2 ** 5,\n",
    "    n_estimators=100\n",
    ")\n",
    "model = model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dropoff_cell_id                           941\n",
       "pickup_cell_id                            631\n",
       "hour                                      423\n",
       "l2_distance                               332\n",
       "weekday                                   214\n",
       "l1_distance                               131\n",
       "cell_pair_count                           113\n",
       "avg_duration_per_cell_pair                 96\n",
       "avg_duration_recent                        95\n",
       "avg_duration_per_weekday_recent            54\n",
       "avg_duration_per_hour_recent               35\n",
       "avg_duration_per_hour_per_cell_pair        23\n",
       "avg_duration_per_weekday_per_cell_pair     12\n",
       "dtype: int32"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default feature importance is based on the number of times a feature is used in the trees. It's not a good metric because it's biased towards categorical features. Categories with many levels are more likely to be used in the trees than categories with few levels.\n",
    "\n",
    "LightGBM provides a better metric called split gain. It's the total gain of each feature when it's used in the trees. It's a better metric because it's not biased towards categorical features. It's also a better metric because it's normalized. It's the total gain divided by the number of times the feature is used in the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l2_distance                               4767436.50\n",
       "dropoff_cell_id                            631979.84\n",
       "hour                                       545120.92\n",
       "avg_duration_per_cell_pair                 318701.27\n",
       "pickup_cell_id                             313141.82\n",
       "weekday                                    214134.00\n",
       "avg_duration_recent                        159161.57\n",
       "l1_distance                                 48904.12\n",
       "avg_duration_per_weekday_recent             48715.68\n",
       "cell_pair_count                             45376.78\n",
       "avg_duration_per_hour_recent                32077.81\n",
       "avg_duration_per_hour_per_cell_pair         30626.98\n",
       "avg_duration_per_weekday_per_cell_pair       8136.40\n",
       "dtype: object"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor(\n",
    "    objective='regression_l1',\n",
    "    num_leaves=2 ** 5,\n",
    "    n_estimators=100,\n",
    "    importance_type='gain'\n",
    "\n",
    ")\n",
    "model = model.fit(X_train, y_train)\n",
    "(\n",
    "    pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "    .sort_values(ascending=False)\n",
    "    .map('{:.2f}'.format)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantiles\n",
    "\n",
    "LightGBM can optimize different loss functions. In particular, it can optimize quantile loss functions. This allows us to output prediction intervals. The idea is to train a model for each quantile. For example, we can train a model for the 0.05 quantile, the 0.5 quantile, and the 0.95 quantile. We can then use these models to output prediction intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.05, 0.5, 0.95]\n",
    "y_pred = pd.DataFrame(index=X_test.index, columns=quantiles)\n",
    "\n",
    "for q in quantiles:\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective='quantile',\n",
    "        alpha=q,\n",
    "        num_leaves=2 ** 5,\n",
    "        n_estimators=100\n",
    "    )\n",
    "    model = model.fit(X_train, y_train)\n",
    "    y_pred[q] = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.05</th>\n",
       "      <th>0.50</th>\n",
       "      <th>0.95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1219332</th>\n",
       "      <td>251.932303</td>\n",
       "      <td>399.497378</td>\n",
       "      <td>727.883352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219333</th>\n",
       "      <td>647.351284</td>\n",
       "      <td>860.229473</td>\n",
       "      <td>1270.267670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219334</th>\n",
       "      <td>596.019179</td>\n",
       "      <td>804.426819</td>\n",
       "      <td>1247.829629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219335</th>\n",
       "      <td>916.289646</td>\n",
       "      <td>1163.106212</td>\n",
       "      <td>1984.173113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219336</th>\n",
       "      <td>394.629415</td>\n",
       "      <td>578.263293</td>\n",
       "      <td>985.190387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0.05         0.50         0.95\n",
       "1219332  251.932303   399.497378   727.883352\n",
       "1219333  647.351284   860.229473  1270.267670\n",
       "1219334  596.019179   804.426819  1247.829629\n",
       "1219335  916.289646  1163.106212  1984.173113\n",
       "1219336  394.629415   578.263293   985.190387"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the prediction intervals contain the true values. In this case, we should expect the model to output prediction intervals that contain the true values 90% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.92%\n"
     ]
    }
   ],
   "source": [
    "in_ci = ((y_pred[0.05] <= y_test) & (y_test <= y_pred[0.95])).mean()\n",
    "print(f'{in_ci:.2%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
