{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF for dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a dataset which was used for a [Kaggle InClass competition](https://www.kaggle.com/competitions/defi-ia-insa-toulouse/overview) from a few years ago. The goal is to predict a person's job based on their resume. The competition's purpose was to build a classifier that was biased towards gender. But in this notebook, we'll just focus on the TF-IDF part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>gender</th>\n",
       "      <th>job</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>She is also a Ronald D. Asmus Policy Entrepre...</td>\n",
       "      <td>F</td>\n",
       "      <td>professor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He is a member of the AICPA and WICPA. Brent ...</td>\n",
       "      <td>M</td>\n",
       "      <td>accountant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dr. Aster has held teaching and research posi...</td>\n",
       "      <td>M</td>\n",
       "      <td>professor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He runs a boutique design studio attending cl...</td>\n",
       "      <td>M</td>\n",
       "      <td>architect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He focuses on cloud security, identity and ac...</td>\n",
       "      <td>M</td>\n",
       "      <td>architect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          description gender         job\n",
       "Id                                                                      \n",
       "0    She is also a Ronald D. Asmus Policy Entrepre...      F   professor\n",
       "1    He is a member of the AICPA and WICPA. Brent ...      M  accountant\n",
       "2    Dr. Aster has held teaching and research posi...      M   professor\n",
       "3    He runs a boutique design studio attending cl...      M   architect\n",
       "4    He focuses on cloud security, identity and ac...      M   architect"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = pathlib.Path('../../data/bias-in-bios.zip')\n",
    "\n",
    "with zipfile.ZipFile(data_dir, 'r') as z:\n",
    "    with z.open('train.json') as f:\n",
    "        train = pd.read_json(f).set_index('Id')\n",
    "    with z.open('categories_string.csv') as f:\n",
    "        names = pd.read_csv(f)['0'].to_dict()\n",
    "    with z.open('train_label.csv') as f:\n",
    "        jobs = pd.read_csv(f, index_col='Id')['Category']\n",
    "        jobs = jobs.map(names)\n",
    "        jobs = jobs.rename('job')\n",
    "        train['job'] = jobs\n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'217,197'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{len(train):,d}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "professor            70016\n",
       "attorney             18820\n",
       "photographer         14646\n",
       "nurse                12622\n",
       "journalist           12295\n",
       "physician            11607\n",
       "psychologist         10391\n",
       "teacher               9145\n",
       "surgeon               6616\n",
       "architect             5841\n",
       "dentist               5450\n",
       "painter               4621\n",
       "poet                  4292\n",
       "filmmaker             4124\n",
       "model                 4115\n",
       "software_engineer     4060\n",
       "composer              3395\n",
       "accountant            3121\n",
       "dietitian             2288\n",
       "comedian              1639\n",
       "pastor                1497\n",
       "chiropractor          1406\n",
       "paralegal              967\n",
       "yoga_teacher           944\n",
       "interior_designer      858\n",
       "dj                     831\n",
       "personal_trainer       807\n",
       "rapper                 783\n",
       "Name: job, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['job'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No machine learning model takes as input text directly. The text always has to be transformed. In particular, for text, the act of transforming text into a vector of numbers is called **vectorization**. There are many ways to vectorize text, but the most common one is called **TF-IDF**. Before we go into that, let's first look at a simpler method called **Bag of Words**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vectorizer does two things. First it normalizes the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' she is also a ronald d. asmus policy entrepreneur fellow with the german marshall fund and is a visiting fellow at the centre for international studies (cis) at the university of oxford. this commentary first appeared at sada, an online journal published by the carnegie endowment for international peace.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = vectorizer.build_preprocessor()(train['description'][0])\n",
    "clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it splits the text into tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she',\n",
       " 'is',\n",
       " 'also',\n",
       " 'ronald',\n",
       " 'asmus',\n",
       " 'policy',\n",
       " 'entrepreneur',\n",
       " 'fellow',\n",
       " 'with',\n",
       " 'the']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = vectorizer.build_tokenizer()(clean)\n",
    "tokens[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is then to build a matrix where each row corresponds to a document and each column corresponds to a token. The value of each cell is the number of times the token appears in the document. This is called a **Bag of Words** representation because we lose the order of the words in the text. We only keep track of the number of times each word appears in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<217197x230368 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9851657 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = vectorizer.fit_transform(raw_documents=train['description'])\n",
    "counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a sparse matrix, because that's a data structure which makes sense in this case: most documents will only contain a small subset of the tokens, so it's a waste of memory to store all the zeros. Sparse matrices are very common in text processing, so some machine learning algorithms are optimized to work with them.\n",
    "\n",
    "It's important to think about the data in terms of a sparse matrix. For instance, regular standard scaling should be avoided. Indeed, if you subtract the mean of a sparse matrix, you'll get a dense matrix, which will take a lot of memory. Instead, you should use a scaler which is aware of the sparse structure of the data, such as `MaxAbsScaler` or `MinMaxScaler`. Indeed, dividing each value by the maximum value of the row will keep the data sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;standardscaler&#x27;, StandardScaler(with_mean=False)),\n",
       "                (&#x27;normalizer&#x27;, Normalizer()),\n",
       "                (&#x27;sgdclassifier&#x27;,\n",
       "                 SGDClassifier(loss=&#x27;log_loss&#x27;, max_iter=100))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;standardscaler&#x27;, StandardScaler(with_mean=False)),\n",
       "                (&#x27;normalizer&#x27;, Normalizer()),\n",
       "                (&#x27;sgdclassifier&#x27;,\n",
       "                 SGDClassifier(loss=&#x27;log_loss&#x27;, max_iter=100))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler(with_mean=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Normalizer</label><div class=\"sk-toggleable__content\"><pre>Normalizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, max_iter=100)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
       "                ('standardscaler', StandardScaler(with_mean=False)),\n",
       "                ('normalizer', Normalizer()),\n",
       "                ('sgdclassifier',\n",
       "                 SGDClassifier(loss='log_loss', max_iter=100))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import pipeline\n",
    "from sklearn import preprocessing\n",
    "\n",
    "model = pipeline.make_pipeline(\n",
    "    CountVectorizer(),\n",
    "    preprocessing.StandardScaler(with_mean=False),\n",
    "    preprocessing.Normalizer(),\n",
    "    linear_model.SGDClassifier(loss='log_loss', max_iter=100, tol=1e-3)\n",
    ")\n",
    "model.fit(train['description'], train['job'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bag of Words representation is very simple, but it has a few drawbacks. First, it doesn't take into account the order of the words. Second, it doesn't take into account the fact that some words are more common than others. For instance, the word \"the\" is very common, but it doesn't carry much information. TF-IDF is a way to fix that.\n",
    "\n",
    "TF-IDF stands for **Term Frequency - Inverse Document Frequency**. It's a way to normalize the Bag of Words representation. The idea is to divide each value by the number of times the token appears in the document. This is called the **Term Frequency**. But we also divide by the number of documents in which the token appears. This is called the **Inverse Document Frequency**. The intuition is that if a token appears in many documents, it's not very informative. On the other hand, if it appears in only a few documents, it's more informative.\n",
    "\n",
    "There are actually many ways to compute the TF-IDF. The most common one is called **smoothed TF-IDF**. It's computed as follows:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(d, t) = \\frac{\\text{TF}(d, t)}{\\text{max}(\\text{TF}(d, t'))} \\times \\log\\left(\\frac{N}{\\text{DF}(t)}\\right)\n",
    "$$\n",
    "\n",
    "where $d$ is a document, $t$ is a token, $t'$ is a token in the document $d$, $N$ is the number of documents, $\\text{TF}(d, t)$ is the number of times the token $t$ appears in the document $d$, $\\text{DF}(t)$ is the number of documents in which the token $t$ appears.\n",
    "\n",
    "The first part is the **Term Frequency**. The second part is the **Inverse Document Frequency**. The $\\log$ is here to make sure that the values are not too large. The $\\text{max}$ is here to make sure that the values are not too small. The $\\text{max}$ is computed over all the tokens in the document $d$.\n",
    "\n",
    "**TLDR: TF-IDF is a way to normalize the Bag of Words representation. It's a way to give more importance to rare words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(raw_documents=train['description'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x230368 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 37 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([160507,  75843,  47025,  44714, 170236, 113882, 154903,  24702,\n",
       "       181876,  26907,  83165,  55279, 207078, 157328, 153755, 214671,\n",
       "        52562, 199296, 108467,  84635,  48901,  29976, 219339,  24992,\n",
       "        86829, 133833,  89664, 206175, 224453,  81756,  76468, 165639,\n",
       "        29401, 179670,  23555, 109703, 189090], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix[0].indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17587834, 0.209706  , 0.18732235, 0.08337532, 0.09836813,\n",
       "       0.11498371, 0.13353337, 0.064915  , 0.31659522, 0.12865291,\n",
       "       0.10931213, 0.21066895, 0.09937576, 0.16555669, 0.03304113,\n",
       "       0.05916193, 0.26172833, 0.10305093, 0.2036051 , 0.09570113,\n",
       "       0.14782546, 0.15956879, 0.15093547, 0.02909765, 0.18124474,\n",
       "       0.19921095, 0.17256723, 0.13280117, 0.05378592, 0.24756103,\n",
       "       0.2139983 , 0.12751501, 0.32776078, 0.22907902, 0.07296681,\n",
       "       0.09074152, 0.05282196])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix[0].data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asmus                0.328\n",
      "sada                 0.317\n",
      "cis                  0.262\n",
      "fellow               0.248\n",
      "ronald               0.229\n",
      "entrepreneur         0.214\n",
      "commentary           0.211\n",
      "endowment            0.210\n",
      "international        0.204\n",
      "marshall             0.199\n",
      "carnegie             0.187\n",
      "fund                 0.181\n",
      "peace                0.176\n",
      "german               0.173\n",
      "oxford               0.166\n",
      "at                   0.160\n",
      "visiting             0.151\n",
      "centre               0.148\n",
      "online               0.134\n",
      "the                  0.133\n",
      "appeared             0.129\n",
      "policy               0.128\n",
      "journal              0.115\n",
      "first                0.109\n",
      "studies              0.103\n",
      "this                 0.099\n",
      "published            0.098\n",
      "for                  0.096\n",
      "is                   0.091\n",
      "by                   0.083\n",
      "also                 0.073\n",
      "an                   0.065\n",
      "university           0.059\n",
      "with                 0.054\n",
      "she                  0.053\n",
      "of                   0.033\n",
      "and                  0.029\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "indices = tfidf_matrix[0].indices\n",
    "scores = tfidf_matrix[0].data\n",
    "\n",
    "for i in scores.argsort()[::-1]:\n",
    "    print(f\"{feature_names[indices[i]]:<20} {scores[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to a Bag of Words representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the                  4.000\n",
      "at                   3.000\n",
      "international        2.000\n",
      "is                   2.000\n",
      "fellow               2.000\n",
      "for                  2.000\n",
      "german               1.000\n",
      "visiting             1.000\n",
      "and                  1.000\n",
      "fund                 1.000\n",
      "marshall             1.000\n",
      "with                 1.000\n",
      "entrepreneur         1.000\n",
      "policy               1.000\n",
      "asmus                1.000\n",
      "ronald               1.000\n",
      "also                 1.000\n",
      "centre               1.000\n",
      "peace                1.000\n",
      "endowment            1.000\n",
      "studies              1.000\n",
      "carnegie             1.000\n",
      "by                   1.000\n",
      "published            1.000\n",
      "journal              1.000\n",
      "online               1.000\n",
      "an                   1.000\n",
      "sada                 1.000\n",
      "appeared             1.000\n",
      "first                1.000\n",
      "commentary           1.000\n",
      "this                 1.000\n",
      "oxford               1.000\n",
      "of                   1.000\n",
      "university           1.000\n",
      "cis                  1.000\n",
      "she                  1.000\n"
     ]
    }
   ],
   "source": [
    "indices = counts[0].indices\n",
    "scores = counts[0].data\n",
    "\n",
    "for i in scores.argsort()[::-1]:\n",
    "    print(f\"{feature_names[indices[i]]:<20} {scores[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing to mention is that the tokenization can be customized. For instance, in search engines, it's common to use n-grams instead of tokens. An n-gram is a sequence of n tokens. In particular, trigrams are quite common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrammer = CountVectorizer(ngram_range=(1, 3))\n",
    "trigrams = trigrammer.fit_transform(raw_documents=train['description'][:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the                  4.000\n",
      "at                   3.000\n",
      "is                   2.000\n",
      "international        2.000\n",
      "for                  2.000\n",
      "for international    2.000\n",
      "fellow               2.000\n",
      "at the               2.000\n",
      "asmus policy         1.000\n",
      "fellow with          1.000\n",
      "entrepreneur fellow  1.000\n",
      "policy entrepreneur  1.000\n",
      "for international peace 1.000\n",
      "ronald asmus         1.000\n",
      "also ronald          1.000\n",
      "the german           1.000\n",
      "is also              1.000\n",
      "she is               1.000\n",
      "peace                1.000\n",
      "endowment            1.000\n"
     ]
    }
   ],
   "source": [
    "feature_names = trigrammer.get_feature_names_out()\n",
    "\n",
    "indices = trigrams[0].indices\n",
    "scores = trigrams[0].data\n",
    "\n",
    "for i in scores.argsort()[::-1][:20]:\n",
    "    print(f\"{feature_names[indices[i]]:<20} {scores[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be done at the character level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrammer = TfidfVectorizer(ngram_range=(1, 3), analyzer='char_wb', )\n",
    "trigrams = trigrammer.fit_transform(raw_documents=train['description'][:10_000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     0.725\n",
      "e                    0.215\n",
      "a                    0.185\n",
      "n                    0.170\n",
      "t                    0.155\n",
      "i                    0.148\n",
      "r                    0.133\n",
      "o                    0.118\n",
      "s                    0.118\n",
      "l                    0.104\n",
      " a                   0.074\n",
      "d                    0.074\n",
      "h                    0.067\n",
      "na                   0.063\n",
      "f                    0.060\n",
      "e                    0.059\n",
      "rna                  0.056\n",
      "smu                  0.056\n",
      "(ci                  0.054\n",
      "nal                  0.054\n",
      "is)                  0.053\n",
      "fel                  0.052\n",
      "rn                   0.052\n",
      "u                    0.052\n",
      "wme                  0.050\n",
      "nt                   0.049\n",
      " f                   0.049\n",
      "owm                  0.049\n",
      "al                   0.048\n",
      "pea                  0.048\n"
     ]
    }
   ],
   "source": [
    "feature_names = trigrammer.get_feature_names_out()\n",
    "\n",
    "indices = trigrams[0].indices\n",
    "scores = trigrams[0].data\n",
    "\n",
    "for i in scores.argsort()[::-1][:30]:\n",
    "    print(f\"{feature_names[indices[i]]:<20} {scores[i]:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
